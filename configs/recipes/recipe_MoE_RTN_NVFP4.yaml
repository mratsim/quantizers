# quantizers/configs/recipes/recipe_moe_nvfp4.yaml
#
# NVFP4 round-to-nearest mixed precision quantization recipe
# Quantizes only MLP expert layers while preserving router/gate, shared experts and self-attention in BF16
#
# This will not work in vLLM until https://github.com/vllm-project/vllm/pull/27608
# You'll need to add a Linear target manually due to:
# - https://github.com/vllm-project/vllm/issues/28197
# - https://github.com/vllm-project/llm-compressor/issues/2163
# 
# NVFP4 round-to-nearest is competitive with more elaborate methods:
# - https://arxiv.org/pdf/2509.23202 - Bridging the Gap Between Promise and Performance for Microscaling FP4 Quantization
# - https://arxiv.org/pdf/2512.02010 - Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling

default_stage:
  default_modifiers:
    QuantizationModifier:
      scheme: NVFP4
      targets:
        # Include only MLP expert weights for quantization
        - "re:.*mlp\\.experts\\.\\d+\\.(down_proj|gate_proj|up_proj)$"
